<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Zach McCormick's Personal Blog - Software Development</title><link href="https://z11k.com/" rel="alternate"></link><link href="https://z11k.com/feeds/software-development.atom.xml" rel="self"></link><id>https://z11k.com/</id><updated>2019-06-20T00:00:00-04:00</updated><entry><title>Connected Content And The Slashdot Effect (Or How I Learned To Scale APIs)</title><link href="https://z11k.com/posts/2019/06/connected-content-and-the-slashdot-effect-or-how-i-learned-to-scale-apis/" rel="alternate"></link><published>2019-06-20T00:00:00-04:00</published><updated>2019-06-20T00:00:00-04:00</updated><author><name>Zach McCormick</name></author><id>tag:z11k.com,2019-06-20:/posts/2019/06/connected-content-and-the-slashdot-effect-or-how-i-learned-to-scale-apis/</id><summary type="html">&lt;p&gt;One of the coolest ways our customers integrate with our product is through a feature we call Connected
Content. By utilizing custom HTTP endpoints—either owned by the customer or via one of their partners—our 
customers can inject customized, on-demand content into messages right before they’re sent. Customers …&lt;/p&gt;</summary><content type="html">&lt;p&gt;One of the coolest ways our customers integrate with our product is through a feature we call Connected
Content. By utilizing custom HTTP endpoints—either owned by the customer or via one of their partners—our 
customers can inject customized, on-demand content into messages right before they’re sent. Customers use
this for a huge variety of use cases, such as personalized recommendations, weather information, and
automated language translations. One of the common hiccups, however, is that these endpoints must be able
to absorb huge spikes in traffic in order for Braze to send messages quickly.&lt;/p&gt;
&lt;p&gt;In the early 2000s, the phenomenon of a web service being unavailable due to large traffic spikes was known
as the Slashdot Effect. Sites linked by popular articles on Slashdot would see huge amounts of traffic very
quickly and almost immediately become unusable—this was just how it was before the days of cheap,
easy-to-deploy auto-scaling and load-balancing services. Even today, this problem persists, and with
regards to Connected Content, we’ve come to see a similar “Braze Effect,” where endpoints originally
designed for lower-volume, consistent traffic are hit with a large traffic spike to inject Connected
Content into messages due to Braze’s sending speed.&lt;/p&gt;
&lt;p&gt;Building something capable of absorbing massive but infrequent traffic spikes can be tricky. All kinds
of strategies can be taken around application server tuning, tactical database choices, and judicious
load testing in order to build something to fit that profile, even at a relatively low cost.&lt;/p&gt;
&lt;p&gt;Check out the rest of this blog post at &lt;a href="https://www.braze.com/perspectives/article/building-braze-job-queues-resiliency"&gt;Building Braze&lt;/a&gt;!&lt;/p&gt;
&lt;!-- COMMENTED OUT TO DRIVE TRAFFIC TO BRAZE

In this post, we’ll dive deeper into this using my favorite framework for building simple web services: Django. It supports multiple databases, cache backends, queueing systems, etc. out of the box. We’ll walk through a step-by-step process of using Django to develop and iterate on a high-performance API capable of absorbing huge traffic spikes. We’ll use Heroku for hosting our service and flood.io for load testing it.

I’m providing instructions on how to do this on recent versions of MacOS using Homebrew, a command line package manager, but feel free to adjust for your platform of choice.

## Installation and Initialization

To get started, you'll need Postgres and Redis installed.

![](scaling-apis-1.png)

Then, clone the example repository and checkout the first step.

![](scaling-apis-2.png)

Once you’ve cloned the example repository, you’ll want to install the requirements and boot up the server.

![](scaling-apis-3.png)

That should get you running locally so you can test that your endpoint works. To get started with Heroku via the CLI tool, create a new application and push your code to it via git.

![](scaling-apis-4.png)

Once you’re set up on Heroku, you should see something like the following when you visit your new application:

![](scaling-apis-5.png)

Go into the Heroku dashboard for your application and you’ll see a screen such as the following:

![](scaling-apis-6.png)

Click on “Configure Dynos” and go ahead and select the 1x Standard Dyno for our default choice. That should be powerful enough for our testing use case right now.

![](scaling-apis-7.png)

Once you’ve done that, you’ll want to modify your Heroku Postgres instance as well. We’ll go ahead and use the Standard-0 size instance. It should allow enough connections and be fast enough not to limit our response time too much.

![](scaling-apis-8.png)

You’ll have two instances of Heroku Postgres for a while. You’ll need to promote the new instance to the default one using a command similar to the one below (note: HEROKU_POSTGRESQL_IVORY may not be the environment variable/name for your particular instance).

![](scaling-apis-9.png)

Once you’ve migrated over to the larger database instance, you can delete the old, smaller one. When you’re done, your dashboard should look something like this.

![](scaling-apis-10.png)

Lastly, we want to turn off debug mode, so go ahead and set the DJANGO_DEBUG environment variable to false.

![](scaling-apis-11.png)

## Our First Test: A Basic Setup

I enjoy using flood.io for load testing—it’s relatively simple to generate huge amounts of load with relatively little configuration. Our first test will be hitting our endpoint with 500 users for five minutes. You’ll want to configure your test to look something like the screenshot below. Note: you may need to purchase credits on flood.io for some of these more intensive tests.

![](scaling-apis-12.png)

Next, launch your test!

![](scaling-apis-13.png)

While your grid is booting and your tests are about to spin up, go back to the Heroku dashboard so we can see what’s going on from your application’s point of view.

![](scaling-apis-14.png)

We’ll alternate between flood.io and your application’s metrics dashboard. Here is approximately what it should look like when your test finishes.

![](scaling-apis-15.png)

We never set up a page, so everything was a 400 error. Average response time of about 600ms, max of about 4 seconds. What next?

Let's make a database model, generate some fake data, then make a page that renders something from the database. To save some time, simply check out and push the next step in the example project.

![](scaling-apis-16.png)

From here, take the following steps locally to confirm it all works.

![](scaling-apis-17.png)

Visit http://127.0.0.1:8000/ and confirm you get a 140-character string returned to you. That’s our “data” to simulate something you might want your Connected Content endpoint to return for each user.

On Heroku’s end, we need to generate the seed data as well. To do that, use the Run Console option and run the following command.

![](scaling-apis-18.png)

![](scaling-apis-19.png)

Visit your application and confirm it works the same as it does locally.

![](scaling-apis-20.png)

## Our Second Test: Serving Real Data

For this test, we’ll use the same size flood.

![](scaling-apis-21.png)

Once you launch the test, you’ll see different (better) results!

![](scaling-apis-22.png)

On the Heroku side of things, we can see that we’re successfully serving the requests now. Fantastic!

![](scaling-apis-23.png)

We set up a page now, so we should be getting all 200s this time. Our average response time was about 47ms, max around 150ms—great! We're probably hitting the table cache every time in the database, so we have some options for pushing our system:

- We could add more concurrent requests
- We could add way more seed data so that everything isn't cached in the database

Let's add more requests!

## Our Third Test: 10x User Count

![](scaling-apis-24.png)

This time we’ll use 5,000 users to simulate 10x the load.

![](scaling-apis-25.png)

This time we had five minutes of about 9k requests per minute with an average response of 14 seconds. We also started to see some failed requests. That’s not great, so let’s think about how we can scale our system a little bit better. We have a couple ways we could scale out now:

- We could add additional dynos
- We could change our server application

We're going to use the second option.

Note: gunicorn, with naive settings, is going to use a pool of workers and doesn't handle concurrent requests very well. Let's install gevent and tell our server process to use greenlets instead of synchronous workers.

Instead of doing that yourself, you can just check out the next step and deploy to Heroku.

![](scaling-apis-26.png)

Visit your application and confirm it still works.

## Our Fourth Test: Tuning the Application Server

![](scaling-apis-27.png)

Wow, what happened? We tried the same 5000 users but our application just couldn’t keep up! The error rate is almost 90%. Something is wrong. Let’s check the Heroku logs.

![](scaling-apis-28.png)

I’ll explain what this means to save you some Googling! A caveat of using asynchronous workers on gunicorn is that each one is going to open its own connection to the database. In our logs, we can see that we're throwing exceptions because there are too many connections.

Looking at the Postgres metrics page in the Heroku dashboard, we can see that we opened FAR more connections than the first time (we were using single-digit connections the first time).

![](scaling-apis-29.png)

Let's try something else. Waitress is a pure-Python WSGI server that will buffer requests to a fixed pool of workers, but still handle incoming HTTP connections in an asynchronous way. Let's install that, and change our Procfile accordingly. As per usual, you can just check out that step.

![](scaling-apis-30.png)

As always, visit your application to confirm it works.

Note: you may need to restart your dynos before deploying to free up database connections.

## Our Fifth Test: A Better Application Server

![](scaling-apis-31.png)

This time we were easily able to handle 5000 concurrent users over five minutes with a 0% error rate and 178ms average response time. AWESOME! Let's add more concurrent requests—we weren't taxing this at all!

## Our Sixth Test: 2x More Users

![](scaling-apis-32.png)

![](scaling-apis-33.png)

With 10k concurrent users over five minutes, we were serving about 108k requests. We probably hit peak concurrency in this range: errors went up and response time spiked to about nine seconds on average.

Let's try scaling up by adding an additional dyno.

![](scaling-apis-34.png)

## Our Seventh Test: Double the Dynos

![](scaling-apis-35.png)

![](scaling-apis-36.png)

This time we also used 10k concurrent users but were able to serve closer to 180k requests. Our response time went back down to 215ms on average. Great! Let's try increasing the load just a little bit more.

## Our Eighth Test: 15k Users

![](scaling-apis-37.png)

This time with 15k concurrent users over five minutes we served about 150k requests and response time went up to 11s. Somewhere in there we've hit peak concurrency again. This next time, though, we'll try a new strategy—let's not hit the database, but let's hit Redis instead. Redis is an in-memory key-value store, so we have to store our data a little bit differently than we might if we were using a relational database. With a relational database, we’d probably be joining normalized tables together to put together everything we need. With Redis, we need to change how we think about storage.

Check out the next step from git and look at the new code.

![](scaling-apis-38.png)

To test this locally, you’ll want to run the following to seed our Redis instance with data.

![](scaling-apis-39.png)

Visit your application locally, confirm it works, then replicate this on Heroku via the Run Console again. You’ll need to add a Redis instance to your application. I used Premium-0 since it can handle the number of incoming connections our two dynos need.

![](scaling-apis-40.png)

## Our Ninth Test: Redis over Postgres

![](scaling-apis-41.png)

![](scaling-apis-42.png)

With 10k concurrent users again (we stepped down to compare fairly against our seventh test), we can see our application handled about 190k requests over five minutes, and response time went down to 34ms. WOW! Let's see if we can handle the 15k users with Redis!

## Our Final Test: Redis with 15k Users

![](scaling-apis-43.png)

![](scaling-apis-44.png)

With 15k concurrent users over five minutes, we were able to serve 255k requests. Response time went back up, but only to 1.4s, and we still had very few failed requests.

At this point, by looking at the Heroku metrics page, we're probably hitting peak concurrency on the dynos (note that their load is extremely high). This shows though, that we can handle nearly 1,000 requests per second with a single Redis instance and two standard dynos. That's pretty good scale for a cheap Heroku deployment!

## Summary

When you’re setting up high-availability low-latency APIs (such as your Connected Content endpoints), consider emulating this model. One way to populate a Redis database with relevant data from a transactional system is to use post-save hooks (if you’re using an ORM like the one built into Django) to save the relevant information to Redis, keyed in a way similar to your Connected Content endpoint’s URL scheme works. That means if you’re hitting https://my-application.com/data?user_id=1234, you might store data for the 1234 user in the key 1234 using HSET. This will be extremely fast to query and will allow you to squeeze the most speed out of the smallest amount of additional infrastructure.

If you’re interested in working at this kind of scale, check out our job board at https://grnh.se/94ab43241. If you want to chat about scale in general, shoot an email to zach.mccormick@braze.com. I’d love to hear your thoughts!

--&gt;</content><category term="software architecture"></category><category term="api design"></category><category term="design patterns"></category><category term="architecture patterns"></category></entry><entry><title>Achieving Resiliency With Queues: Building A System That Never Skips A Beat In A Billion</title><link href="https://z11k.com/posts/2018/12/achieving-resiliency-with-queues-building-a-system-that-never-skips-a-beat-in-a-billion/" rel="alternate"></link><published>2018-12-20T00:00:00-05:00</published><updated>2018-12-20T00:00:00-05:00</updated><author><name>Zach McCormick</name></author><id>tag:z11k.com,2018-12-20:/posts/2018/12/achieving-resiliency-with-queues-building-a-system-that-never-skips-a-beat-in-a-billion/</id><summary type="html">&lt;p&gt;Braze processes billions and billions of events per day on behalf of its customers, resulting in billions
of hyper-focused, personalized messages—but failing to send one of those messages has consequences. To make
sure those key messages are always correct and always on time, Braze takes a strategic approach to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Braze processes billions and billions of events per day on behalf of its customers, resulting in billions
of hyper-focused, personalized messages—but failing to send one of those messages has consequences. To make
sure those key messages are always correct and always on time, Braze takes a strategic approach to how we
leverage job queues.&lt;/p&gt;
&lt;h2 id="whats-a-job-queue"&gt;What’s a Job Queue?&lt;/h2&gt;
&lt;p&gt;A typical job queue is an architectural pattern where processes submit computation jobs to a queue and
other processes actually execute the jobs. This is usually a good thing—when used properly, it gives you
degrees of concurrency, scalability, and redundancy that you can’t get with a traditional request–response
paradigm. Many workers can be executing different jobs simultaneously in multiple processes, multiple
machines, or even multiple data centers for peak concurrency. You can assign certain worker nodes to work
on certain queues and send particular jobs to specific queues, allowing you to scale resources as needed.
If a worker process crashes or a data center goes offline, other workers can execute the remaining jobs.&lt;/p&gt;
&lt;p&gt;While you can certainly apply these principles and run a job-queueing system easily at a small scale, the
seams start to show (and even burst) when you’re processing billions and billions of jobs. Let’s take a
look at a few problems Braze has faced as we’ve grown from processing thousands, to millions, and now
billions of jobs per day.&lt;/p&gt;
&lt;p&gt;Check out the rest of this blog post at &lt;a href="https://www.braze.com/perspectives/article/building-braze-job-queues-resiliency"&gt;Building Braze&lt;/a&gt;!&lt;/p&gt;
&lt;!-- COMMENTED OUT TO DRIVE TRAFFIC TO BRAZE
## Lack of Consistency Is a Weakness

What happens if we send a message, but we crash before recording the fact that we just sent that message?

A couple different bad outcomes are possible here. First, you might reschedule the failed job and send the message again. That’s…not ideal: no one wants to receive the same thing twice. Instead, consider not rescheduling it at all. In that case, our internal accounting will be incorrect, so attributions, conversions, and all kinds of other things won’t be right moving forward.

How do we fix that? When writing our job definitions, we think really hard about idempotency and retry behavior.

When you’re talking about queues, idempotency means that a single job can be terminated at an arbitrary point, the re-queued job reran in its entirety, and the end result will be the same as if we had successfully run the job exactly one time. This is intimately tied to our retry behavior of choice—at-least-once delivery. By keeping in mind that all of our jobs will be run at least once, and maybe multiple times, we can write idempotent job definitions that ensure consistency even in the face of random failures.

Going back to our message-sending example, how might we use these concepts to ensure consistency? In this case, we might break the job into two pieces, with the first one sending the message and enqueuing the second one, and the second one writing to the database. In that scenario, we can retry either job as many times as we want—if the message-sending provider is down, or the internal accounting database is down, we’ll appropriately retry until we succeed!

![](resiliency-1.jpg)

## Good Fences Make Good Neighbors

What happens to our example company Consolidated Widgets’ data processing when the database for Global Gizmos is down?

In this scenario, if our at-least-once delivery strategy is in play, we would expect all of the data-processing jobs for Global Gizmos to retry over and over until they succeed. This is great—we won’t lose any data even while their database is down. For Consolidated Widgets, however, it may not be that great: if the workers are constantly retrying and failing, they might be too busy to process Consolidated Widgets’ work in a timely fashion.

We can fix this by using well-chosen queue names and pausing certain queues as needed. With this in our toolbelt, we can relieve the strain on pieces of infrastructure in a surgical manner. In our scenario above, once we know that Global Gizmos’ database is down, we can pause their data processing queue until we know it’s back up, ensuring that one specific outage doesn’t impact any other customers!

![](resiliency-2.jpg)

## Waiting Is Painful

What if Consolidated Widgets and Global Gizmos send email campaigns to 50 million users each, 5 minutes apart? Who goes first?

Simple job-queuing systems have a simple "work" queue that workers pull jobs from. Once you have a nice variety of different jobs and job types, you probably move on to having multiple types of queues, each having different priorities or types of workers pulling from those queues. In that vein, we have a variety of simple queues for data processing, messaging, and various maintenance tasks.

Fast-forward to when you're sending billions of personalized messages per day, one "messaging" queue isn't going to cut it—what happens when that queue grows extremely large, like in our example above? Do we prioritize the jobs that arrived first?

Our dynamic queueing system seeks to address a phenomenon called job starvation, where a job that is ready to execute waits for a long time before executing, usually because of some kind of priority. In a simple "messaging" queue, the priority is simply the time the job entered the queue, meaning that jobs added to the end of a big queue can end up waiting for a very long time.

When we go to queue up a campaign and all of its messages, instead of adding the jobs to a big "messaging" queue, we create a totally new queue for just this campaign, complete with a special name so we know what it is and how to find it. After adding the jobs to the queue, we grab our “dynamic queues” list and add this new queue name to the end.

By employing this strategy, we can instruct workers to pick up the name of a dynamic queue from the “dynamic queues” list, then process all of the jobs on that particular queue. This lets us ensure that messages are being sent as fast as possible AND that all of our customers are being treated with equal priority.

Consequently, this has other benefits, like higher cache hit rates and fewer database connections, because of the increase in work locality for particular workers. Everybody wins!

![](resiliency-3.jpg)

## Always Have a Backup Plan

What happens when a database is down, some queues are paused, and the job queues start to fill up?

Sometimes important pieces of infrastructure simply die on you. We have secondaries and backups in place, but the time it takes to promote backup infrastructure is almost never zero. Having multiple layers of queues across the entire application infrastructure can be very helpful in mitigating the impact of these types of events.

One such strategy we employ is queueing on devices themselves. Millions and millions of devices have different applications using a Braze SDK, and in those applications, we utilize a queue for sending data to our APIs.

When our SDK goes to submit that data and fails, for whatever reason, the SDK queues up a retry using an exponential backoff algorithm until it succeeds. This strategy minimizes the impact of infrastructure or code failures, since devices will simply queue up their own data and send it to Braze when everything is back online.

![](resiliency-4.jpg)

## Moving Fast and Not Breaking Things

At the end of the day, our goal is to send hyper-focused, personalized messages better than anyone else, and that involves moving quickly, being resilient, and getting everything right. Job queues are at the heart of Braze’s infrastructure, so we’re always watching our performance, employing best practices, and experimenting with new strategies and advanced techniques to be the best in the game.

If this type of high-performance, low-latency systems engineering in the marketing automation space excites you, then you should definitely check our job board!
--&gt;</content><category term="software architecture"></category><category term="job queues"></category><category term="design patterns"></category><category term="architecture patterns"></category></entry><entry><title>Code Vision</title><link href="https://z11k.com/posts/2018/06/code-vision/" rel="alternate"></link><published>2018-06-18T00:00:00-04:00</published><updated>2018-06-18T00:00:00-04:00</updated><author><name>Zach McCormick</name></author><id>tag:z11k.com,2018-06-18:/posts/2018/06/code-vision/</id><summary type="html">&lt;p&gt;While I'm currently on an early flight from Indianapolis to Newark, I'd love to expound on
some thoughts I've had recently about what I call "code vision". I define this as a written,
iterated-on plan for what a codebase should look like in the future. This is different from
what …&lt;/p&gt;</summary><content type="html">&lt;p&gt;While I'm currently on an early flight from Indianapolis to Newark, I'd love to expound on
some thoughts I've had recently about what I call "code vision". I define this as a written,
iterated-on plan for what a codebase should look like in the future. This is different from
what features are in the pipeline - think more of a big-picture, directional plan for the
architecture. I believe that this holistic view is needed as a codebase accumulates age and
a significant number of active developers, much like a past-startup-stage company needs
vision and values. &lt;em&gt;I believe the process of refactoring to be largely ineffective without
first having this vision&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;As a former Boy Scout, I have heard the phrase "Try and leave this world a little better
than you found it" by Robert Baden-Powell more times than I can count, and I try to remember
it in many different contexts - code included. I think that this is easier to apply with
nature and the outdoors - "better" is quite clear to us: reduce waste, don't pollute, and
try not to disturb the wildlife. Software is a bit different, however, in that "better" is
often a very subjective thing. Refactoring - the process of leaving the codebase a little
better than you found it - can be quite aimless unless we have a common concept of "better".&lt;/p&gt;
&lt;p&gt;Imagine that you have a class that is ugly - it has one gigantic method. You break
it up into a series of smaller functions that are easier to test, and call them procedurally
from one "master" method or function. Your comments make it clearer what is going on, but
ultimately the class performs the same function. If you have several classes with similar
structure - one big method or several small, well-commented methods - all wired together
without a class hierarchy or unifying framework, you've only put lipstick on a pig. Your
system itself is still disorganized, so you find some commonalities between parts - say
you are rendering HTML pages and you create a base &lt;code&gt;Controller&lt;/code&gt; class to take care of
authorization, or you are processing SQL-driven reports, so you create a &lt;code&gt;SqlReport&lt;/code&gt;
base class and rewrite several classes to use this new class hierarchy. Without a holistic
vision, it's likely that the next person to work on rendering pages or generating reports
will do their own thing or simply pattern match (i.e. copy, paste, and modify) without
thinking about how their additions or changes fit into the greater picture. This leads to
JSON API controllers using a base &lt;code&gt;Controller&lt;/code&gt; class geared toward rendering HTML and
large bits of transactional SQL statements using a &lt;code&gt;SqlReport&lt;/code&gt; class geared toward
asynchronous queries run on a read replica.&lt;/p&gt;
&lt;p&gt;I have worked at a number of companies, but most of them had relatively young codebases,
with fewer than 20 or 30 developers and fewer than 5 years of history. I've had, however,
the benefit of working on contracts or patent cases where I've been privy to much, much
larger and older codebases as well, and have seen ones with clear and obvious vision, as
well as ones without. Needless to say, the ones without were the least organized, the
least understandable, and the hardest to understand. The ones with vision tended to share
several common, positive values: &lt;em&gt;clarity, modularization, and simplicity&lt;/em&gt;. Moreover,
they tended to exhibit what I refer to as "&lt;strong&gt;framework-oriented thinking&lt;/strong&gt;" - an
architectural approach that favors frameworks written by architects and senior
engineers with components and extensions built by everyone. These are inherently
modular, and often were surprisingly simple and clear. One only had to read the docs
for the major framework components - often base classes or abstract classes that
made up a pipeline, a composite rendering framework, or a communication paradigm -
in order to understand the big picture. From there, any individual components made
sense. It is easy to understand the purpose and function of &lt;code&gt;FanOutPipelineTask&lt;/code&gt; or
&lt;code&gt;class ColoredButton extends Button (extends UIComponent)&lt;/code&gt;, and one couldn't (without
significant effort, anyway) try to shim a subclass of &lt;code&gt;SqlReport&lt;/code&gt; to run on the
write primary!&lt;/p&gt;
&lt;p&gt;New development aside, with this level of increased clarity and understanding, the process
of refactoring changes from a cloudy, somewhat aimless task to one that has purpose and is
rather easy to understand. With a code vision and "framework-oriented thinking", one can
evaluate methods, classes, and components and ask themselves "does this component fit into
the paradigm of the framework as is?" and if the answer is no, then ask "should this
component change, or should the framework change?". Methods can still be divided into
less complex sub-methods, but now classes and components can be changed with purpose - to
better fit into the rest of the system. Code review changes as well - a reviewer can now
ask "do these additions or changes fit in with the framework, or change the framework
itself to better suit its purpose?" rather than simply "does this code function properly
and is it tested appropriately?".&lt;/p&gt;
&lt;p&gt;I'd love to hear your thoughts on this concept. Shoot me an email at
&lt;a href="mailto:z@z11k.com"&gt;z@z11k.com&lt;/a&gt; or find me on LinkedIn - I'm a sucker for organization
and structure, but I love hearing the other side as well - maybe you have a good reason
that such vision and structure hinders feature progress or is overly prescriptive. Let
me know and let's discuss to make the software world a better place!&lt;/p&gt;</content><category term="software architecture"></category><category term="engineering management"></category><category term="teamwork"></category><category term="refactoring"></category><category term="change"></category></entry><entry><title>Running GeoDjango on Heroku (August 2017)</title><link href="https://z11k.com/posts/2017/08/running-geodjango-on-heroku-august-2017/" rel="alternate"></link><published>2017-08-16T00:00:00-04:00</published><updated>2017-08-16T00:00:00-04:00</updated><author><name>Zach McCormick</name></author><id>tag:z11k.com,2017-08-16:/posts/2017/08/running-geodjango-on-heroku-august-2017/</id><summary type="html">&lt;h3 id="heroku-setup"&gt;Heroku Setup&lt;/h3&gt;
&lt;p&gt;There are currently two ways to get the GDAL, GEOS, and PROJ libraries needed by GeoDjango onto your Heroku slug&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Via a custom buildpack
(&lt;a href="https://github.com/cyberdelia/heroku-geo-buildpack"&gt;https://github.com/cyberdelia/heroku-geo-buildpack&lt;/a&gt;) 1.
Via setting the environment variable BUILD_WITH_GEO_LIBRARIES=1 while using the default Python buildpack&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This has two issues as …&lt;/p&gt;</summary><content type="html">&lt;h3 id="heroku-setup"&gt;Heroku Setup&lt;/h3&gt;
&lt;p&gt;There are currently two ways to get the GDAL, GEOS, and PROJ libraries needed by GeoDjango onto your Heroku slug&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Via a custom buildpack
(&lt;a href="https://github.com/cyberdelia/heroku-geo-buildpack"&gt;https://github.com/cyberdelia/heroku-geo-buildpack&lt;/a&gt;) 1.
Via setting the environment variable BUILD_WITH_GEO_LIBRARIES=1 while using the default Python buildpack&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This has two issues as of 8–16–2017: The current “stack” on Heroku is heroku-16.  This stack
does not properly copy over libjasper, which is a required linked library by libgdal. One solution is
to set your stack to cedar-14 instead.  Another solution is to use a slightly modified custom buildpack
(&lt;a href="https://github.com/dschep/heroku-geo-buildpack"&gt;https://github.com/dschep/heroku-geo-buildpack&lt;/a&gt;)
or a slightly modified offical buildpack
([https://github.com/TrailblazingTech/heroku-buildpack-python#fix-gdal-jasper]
(https://github.com/TrailblazingTech/heroku-buildpack-python#fix-gdal-jasper)).&lt;/p&gt;
&lt;p&gt;If you stick with the heroku-16 stack, you’ll want to add the following to your settings.py:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;GDAL_LIBRARY_PATH = os.getenv(‘GDAL_LIBRARY_PATH’)&lt;br&gt; GEOS_LIBRARY_PATH = &amp;gt; os.getenv(‘GEOS_LIBRARY_PATH’)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Then, if using the official Python buildpack, you will want to set your environment variables like so:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;GDAL_LIBRARY_PATH=/app/.heroku/vendor/lib/libgdal.so&lt;br&gt; &amp;gt; GEOS_LIBRARY_PATH=/app/.heroku/vendor/lib/libgeos_c.so&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This may be required as well on cedar-14 — I didn’t take notes during that part of the struggle :-)&lt;/p&gt;
&lt;h3 id="django-settings-changes"&gt;Django Settings Changes&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Add ‘django.contrib.gis’ to your INSTALLED_APPS 1.  Change your DATABASES dict to use postgis, like so:&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;DATABASES[‘default’] = dj_database_url.config(conn_max_age=600,&lt;br&gt; &amp;gt;
default=’postgis://localhost:5432/{}’.format(APP_NAME))&lt;br&gt;  # VERY IMPORTANT BECAUSE HEROKU WILL USE
‘postgres’ AS THE SCHEME&lt;br&gt; DATABASES[‘default’][‘ENGINE’] = ‘django.contrib.gis.db.backends.postgis’&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="other-django-things"&gt;Other Django Things&lt;/h3&gt;
&lt;p&gt;In urls.py, change &lt;strong&gt;from django.contrib import admin&lt;/strong&gt; to &lt;strong&gt;from django.contrib.gis import admin&lt;/strong&gt; for admin urls&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In whatever app makes sense, add the following migration:&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;from django.contrib.postgres.operations import CreateExtension&lt;br&gt; from &amp;gt; django.db import migrations&lt;/p&gt;
&lt;p&gt;class Migration(migrations.Migration):&lt;br&gt;  operations = [&lt;br&gt; &amp;gt; CreateExtension(‘postgis’)&lt;br&gt;  ]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="credits"&gt;Credits&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://twitter.com/cyberdelia"&gt;cyberdelia&lt;/a&gt; and &lt;a href="https://twitter.com/dschep"&gt;dschep&lt;/a&gt; on GitHub for helping provide
and work through the gdal dependency issues with a custom buildpack.&lt;/p&gt;
&lt;h3 id="github-example"&gt;GitHub Example&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/TrailblazingTech/django-gis-heroku"&gt;https://github.com/TrailblazingTech/django-gis-heroku&lt;/a&gt;&lt;/p&gt;</content><category term="heroku"></category><category term="django"></category><category term="gis"></category><category term="geography"></category><category term="devops"></category></entry><entry><title>Tags for Sentry for Android</title><link href="https://z11k.com/posts/2017/02/tags-for-sentry-for-android/" rel="alternate"></link><published>2017-02-28T00:00:00-05:00</published><updated>2017-02-28T00:00:00-05:00</updated><author><name>Zach McCormick</name></author><id>tag:z11k.com,2017-02-28:/posts/2017/02/tags-for-sentry-for-android/</id><summary type="html">&lt;p&gt;I could not figure out how to add tags or anything to my captured exceptions for Android, which led them to be
rather bland/unhelpful when trying to pin down what was causing a bug on the Android camera (notorious for being
inconsistent across manufacturers, devices, Android versions, etc.)&lt;/p&gt;
&lt;p&gt;I …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I could not figure out how to add tags or anything to my captured exceptions for Android, which led them to be
rather bland/unhelpful when trying to pin down what was causing a bug on the Android camera (notorious for being
inconsistent across manufacturers, devices, Android versions, etc.)&lt;/p&gt;
&lt;p&gt;I ended up doing the following, which hopefully helps others!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;com.getsentry.raven.android.Raven&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;com.getsentry.raven.event.EventBuilder&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;com.getsentry.raven.event.helper.EventBuilderHelper&lt;/span&gt;

&lt;span class="o"&gt;...&lt;/span&gt;

&lt;span class="n"&gt;Raven&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;this&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getApplicationContext&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;span class="k"&gt;try&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;Field&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Raven&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;class&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getDeclaredField&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;raven&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setAccessible&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;true&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="n"&gt;com&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getsentry&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;raven&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Raven&lt;/span&gt; &lt;span class="n"&gt;raven&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;com&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getsentry&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;raven&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Raven&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Raven&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;class&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="n"&gt;final&lt;/span&gt; &lt;span class="n"&gt;ConnectivityManager&lt;/span&gt; &lt;span class="n"&gt;connManager&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ConnectivityManager&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;getSystemService&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;CONNECTIVITY_SERVICE&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="n"&gt;final&lt;/span&gt; &lt;span class="n"&gt;NetworkInfo&lt;/span&gt; &lt;span class="n"&gt;mWifi&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;connManager&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getNetworkInfo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ConnectivityManager&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TYPE_WIFI&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="n"&gt;final&lt;/span&gt; &lt;span class="n"&gt;PackageInfo&lt;/span&gt; &lt;span class="n"&gt;pInfo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;getPackageManager&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getPackageInfo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;getPackageName&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;);;&lt;/span&gt;
  &lt;span class="n"&gt;raven&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;addBuilderHelper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;EventBuilderHelper&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nd"&gt;@Override&lt;/span&gt;
      &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;void&lt;/span&gt; &lt;span class="n"&gt;helpBuildingEvent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EventBuilder&lt;/span&gt; &lt;span class="n"&gt;eventBuilder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;eventBuilder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;withTag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;wifi&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;valueOf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mWifi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isConnected&lt;/span&gt;&lt;span class="p"&gt;()));&lt;/span&gt;
        &lt;span class="n"&gt;eventBuilder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;withTag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;app_version&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pInfo&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;versionName&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;eventBuilder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;withTag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;build_version&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Build&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;VERSION&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;RELEASE&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;eventBuilder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;withTag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;build_manufacturer&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Build&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MANUFACTURER&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;eventBuilder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;withTag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;build_brand&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Build&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BRAND&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;eventBuilder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;withTag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;build_device&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Build&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DEVICE&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;eventBuilder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;withTag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;build_product&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Build&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PRODUCT&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;eventBuilder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;withTag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;debug_app&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;BuildConfig&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DEBUG&lt;/span&gt; &lt;span class="err"&gt;?&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;true&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;false&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;eventBuilder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;withTag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;build_brand&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Build&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BRAND&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;});&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="n"&gt;catch&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ne"&gt;Exception&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;Raven&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;capture&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Obviously you can see the pattern here to add tags using the eventBuilder. I have no idea why this was omitted in
the Android version of Raven, but reflection saves the day per usual.&lt;/p&gt;</content><category term="software development"></category><category term="android"></category></entry><entry><title>GitHub Fork and PR Model — “Unknown Repository”</title><link href="https://z11k.com/posts/2016/04/github-fork-and-pr-model-unknown-repository/" rel="alternate"></link><published>2016-04-21T00:00:00-04:00</published><updated>2016-04-21T00:00:00-04:00</updated><author><name>Zach McCormick</name></author><id>tag:z11k.com,2016-04-21:/posts/2016/04/github-fork-and-pr-model-unknown-repository/</id><summary type="html">&lt;p&gt;Have you ever had a PR that you can’t quite merge because it needs changes, but the user has gone away (either
deleted their account/branch/fork or are no longer a collaborator on a project)? Do you see “unknown repository”
when you pull up the PR on GitHub …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Have you ever had a PR that you can’t quite merge because it needs changes, but the user has gone away (either
deleted their account/branch/fork or are no longer a collaborator on a project)? Do you see “unknown repository”
when you pull up the PR on GitHub?&lt;/p&gt;
&lt;p&gt;Fear no more — you can salvage that code! Add the following line to the origin block (or whatever remote you
are using — be sure to change to refs/remotes/REMOTE/pr/*) in /.git/config.&lt;/p&gt;
&lt;p&gt;fetch = +refs/pull/&lt;em&gt;/head:refs/remotes/origin/pr/&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Then you can do git checkout pr/### to grab their code, then git checkout -b my-new-branch to create a local branch
out of the once-lost PR!&lt;/p&gt;
&lt;p&gt;Pretty neat, eh?&lt;/p&gt;</content><category term="software development"></category><category term="git"></category><category term="github"></category></entry><entry><title>Docker, Ubuntu 14.04, and apt-show-versions</title><link href="https://z11k.com/posts/2015/10/docker-ubuntu-1404-and-apt-show-versions/" rel="alternate"></link><published>2015-10-14T00:00:00-04:00</published><updated>2015-10-14T00:00:00-04:00</updated><author><name>Zach McCormick</name></author><id>tag:z11k.com,2015-10-14:/posts/2015/10/docker-ubuntu-1404-and-apt-show-versions/</id><summary type="html">&lt;p&gt;If you’re using docker and the ubuntu:14.04 image (and possibly other images as well), then you’ll want to
follow the following steps to get apt-show-versions to work, which is an essential tool if you want perfectly
reproducible builds/deploys (so that you can figure out exactly …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you’re using docker and the ubuntu:14.04 image (and possibly other images as well), then you’ll want to
follow the following steps to get apt-show-versions to work, which is an essential tool if you want perfectly
reproducible builds/deploys (so that you can figure out exactly what versions to provide to apt-get!)&lt;/p&gt;
&lt;p&gt;If you first install apt-show-versions and get something like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Errors were encountered while processing:
apt-show-versions
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then if you try to run it and get:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@docker-host:/# apt-show-versions python3
Error: No information about packages! (Maybe no deb entries?)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It’s likely that the ubuntu:14.04 docker image has taken some space-saving shortcuts that won’t allow
apt-show-versions to read the dpkg index. Let’s fix that!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@docker-host:/# rm /etc/apt/apt.conf.d/docker-gzip-indexes
root@docker-host:/# apt-get update
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That should fix it! Next time you run &lt;code&gt;apt-show-versions&lt;/code&gt; it should work just fine!&lt;/p&gt;</content><category term="software development"></category><category term="docker"></category><category term="ubuntu"></category></entry><entry><title>Benchmarking Django Model Saving</title><link href="https://z11k.com/posts/2015/07/benchmarking-django-model-saving/" rel="alternate"></link><published>2015-07-07T00:00:00-04:00</published><updated>2015-07-07T00:00:00-04:00</updated><author><name>Zach McCormick</name></author><id>tag:z11k.com,2015-07-07:/posts/2015/07/benchmarking-django-model-saving/</id><summary type="html">&lt;p&gt;When using Django’s ORM as a basis for data harvesting (probably not the best idea in the first place, but
sometimes it’s easier to go with what you know), I learned that Django isn’t exactly crazy fast when saving
models to the database. Here are some stats …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When using Django’s ORM as a basis for data harvesting (probably not the best idea in the first place, but
sometimes it’s easier to go with what you know), I learned that Django isn’t exactly crazy fast when saving
models to the database. Here are some stats on doing different things with Django to try to speed it up.&lt;/p&gt;
&lt;p&gt;First, I wrote two methods and used cProfile to profile them. MyModel is a model with a single max_length=100
CharField.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def benchmark():
 for i in range(1, 10000): x = MyModel(data=”Some Random Data”) x.save()

def benchmark_bulk():
 items = [] for i in range(1, 10000): x = MyModel(data=”Some Random Data”) items.append(x)
 MyModel.objects.bulk_create(items)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I used cProfile to profile these methods, using SQLite as the backing database. benchmark() took 14.87 seconds,
and benchmark_bulk() took 0.400 seconds. Obvious improvement there by using bulk_create, but you can’t use FKs
to link subrecords to the records, as the pk property of those objects will not get set.&lt;/p&gt;
&lt;p&gt;When switching to MySQL: 8.09 seconds on benchmark() and 0.379 seconds on benchmark_bulk(). A little bit better
— definitely better on benchmark, but not that much better when bulk inserting.&lt;/p&gt;
&lt;p&gt;When switching to Postgres: 6.71 seconds on benchmark() and 0.385 seconds on benchmark_bulk(). Even faster on
benchmark, but not any faster for bulk insertion. Bulk insertion may be effectively capped out on speed. The
bulk_create method on query.py seems to take about .2 seconds no matter the backend.&lt;/p&gt;
&lt;p&gt;Switching to Cassandra led to bulk_create not working, and benchmark() took 17.88 seconds! That wasn’t what
I expected here! Looking at the profiler stats, it seems it spent 11.63 seconds in time.sleep(), so maybe I’m
doing something wrong — subtracting these two gives 6.25 seconds, which is closer to what I’d expect.&lt;/p&gt;
&lt;p&gt;Obviously the use cases here are very simple, but for simple model insertion speed, Postgres seems to win if
you want a relational database and migrating to new software isn’t hard. If you don’t want to use Postgres
(in our case, we like MySQL), or you need intense speedups, then modify your code to use bulk_create()!&lt;/p&gt;
&lt;p&gt;I’ll make another post later this week or next week, with a more complicated, real-world data structure, and show
how (likely) Cassandra would beat out both MySQL and Postgres for saving more complex models quickly, since we can
eliminate FK relationships and save objects in a more NoSQL fashion. This comes at a querying and filtering cost,
so I’ll examine those too!&lt;/p&gt;</content><category term="software development"></category><category term="django"></category><category term="benchmarking"></category><category term="databases"></category></entry><entry><title>Wrestling with Python on OS X (Yosemite)</title><link href="https://z11k.com/posts/2015/03/wrestling-with-python-on-os-x-yosemite/" rel="alternate"></link><published>2015-03-01T00:00:00-05:00</published><updated>2015-03-01T00:00:00-05:00</updated><author><name>Zach McCormick</name></author><id>tag:z11k.com,2015-03-01:/posts/2015/03/wrestling-with-python-on-os-x-yosemite/</id><summary type="html">&lt;p&gt;So I have been using my Macbook Pro for about 3 years for various projects, mostly in Java/Android, some Node here
and there, I’ve installed Ruby and RVM for whatever reason once or twice, and I probably have 8 version of Python
installed. I started to work on …&lt;/p&gt;</summary><content type="html">&lt;p&gt;So I have been using my Macbook Pro for about 3 years for various projects, mostly in Java/Android, some Node here
and there, I’ve installed Ruby and RVM for whatever reason once or twice, and I probably have 8 version of Python
installed. I started to work on an open source Python project that I’d been working on (started on a different
computer), and noticed some funky things about my Python install on my MBP. I was constantly getting errors like
the following when I tried to install virtualenv and virtualenvwrapper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;IOError: [Errno 13] Permission denied: ‘/usr/local/bin/virtualenv’&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Obviously at some point in time, over a few version of OSX and lots of Homebrewing, I’d screwed up my
permissions. Let’s fix it!&lt;/p&gt;
&lt;p&gt;First I went ahead and did a ‘brew update &amp;amp;&amp;amp; brew upgrade’. I then did a ‘brew uninstall python3’ and ‘brew
uninstall python’ just to clean everything up — that way I’d only have the OS X Yosemite default system Python
installed. Next I reinstalled Python (2.7.9 when I did it) with ‘brew install python — framework’. Homebrewed
Python comes with pip, so I did ‘pip list’. Hmmm…. that’s funny! I have a ton of crap listed as installed,
but I definitely just did a clean install.&lt;/p&gt;
&lt;p&gt;It turns out that pip will look in a number of different folders, ‘/Library/Python/2.7/site-packages/’ being
one of them. All of the packages I had installed previously using the system Python, such as awscli and goobook,
were all sitting here. The Homebrewed version of pip is listing them though. This definitely isn’t what I want…&lt;/p&gt;
&lt;p&gt;I did an ‘ls’ in the ‘/Library/Python/2.7/site-packages/’ folder and did a ‘sudo pip uninstall xyz’
for every folder such as ‘xyz-1.2.3’ in that directory, excluding pip and setuptools. I then did a ‘sudo rm
-rf’ for the pip and setuptools folders in this directory (just to make sure they don’t cause problems). This
left me with only easy_install.py and pkg_resources.py in that directory.&lt;/p&gt;
&lt;p&gt;Now let’s install virtualenv. ‘pip install virtualenv’ and ‘pip install virtualenvwrapper’
now both work perfectly without needing to use ‘sudo’ in front, because pip is attempting to install
everything and link against existing packages in ‘/usr/local/lib/python2.7/site-packages’ instead of in
‘/Library/Python/2.7/site-packages’. Yay! That’s exactly what we want. I reinstalled awscli and other
Python tools that I had gotten used to using, and pip installed them all correctly into the Homebrewed Python’s
site-packages folder and linked all executables to ‘/usr/local/bin’. Exactly what we want to make it modular
and manageable, and especially not to require ‘sudo’. Great success!&lt;/p&gt;</content><category term="software development"></category><category term="osx"></category><category term="macos"></category><category term="python"></category><category term="virtualenv"></category></entry><entry><title>Self-Extracting Bash Patcher</title><link href="https://z11k.com/posts/2014/02/self-extracting-bash-patcher/" rel="alternate"></link><published>2014-02-16T00:00:00-05:00</published><updated>2014-02-16T00:00:00-05:00</updated><author><name>Zach McCormick</name></author><id>tag:z11k.com,2014-02-16:/posts/2014/02/self-extracting-bash-patcher/</id><summary type="html">&lt;p&gt;Hey all,&lt;/p&gt;
&lt;p&gt;I haven’t posted anything cool in a while, so here is a cool bash shell script I wrote a while back to generate
a self-extracting patch (specifically for source code repos). It uses diff to generate output that can be fed into
patch, then uses grep and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Hey all,&lt;/p&gt;
&lt;p&gt;I haven’t posted anything cool in a while, so here is a cool bash shell script I wrote a while back to generate
a self-extracting patch (specifically for source code repos). It uses diff to generate output that can be fed into
patch, then uses grep and sed to do some formatting as well as find all of the binary files that differ. The file
list is piped to tar and gzipped, then the whole thing is jammed into a single, self-extracting bash script with
the ability to do an initial dry run, as well as to revert all of the changes. I’ve found that sending entire
source trees (20+ GB across 300+ git repos) can be inefficient if the recipient has the original code (he/she just
needs your changes) — sending a 100 MB patch file is much easier!&lt;/p&gt;
&lt;p&gt;Here it is below — let me know if you can think of anything to improve it! You can also check it out or fork it
on GitHub!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/zachmccormick/PatchGenerator"&gt;https://github.com/zachmccormick/PatchGenerator&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here is a link to the original article that I read a while back when I was first interested in
making self-extracting bash scripts.  &lt;a href="http://www.linuxjournal.com/node/1005818"&gt;&lt;strong&gt;Bash Self-Extracting Script | Linux Journal&lt;/strong&gt; &lt;em&gt;In this post
I'll show you how to create a self extracting bash script to automate the installation of files on
your…&lt;/em&gt;www.linuxjournal.com&lt;/a&gt;&lt;/p&gt;</content><category term="software development"></category><category term="github"></category><category term="open-source"></category></entry><entry><title>Where do Android components run?</title><link href="https://z11k.com/posts/2013/10/where-do-android-components-run/" rel="alternate"></link><published>2013-10-27T00:00:00-04:00</published><updated>2013-10-27T00:00:00-04:00</updated><author><name>Zach McCormick</name></author><id>tag:z11k.com,2013-10-27:/posts/2013/10/where-do-android-components-run/</id><summary type="html">&lt;p&gt;If you ever wondered where the different Android components run by default:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Activity — always runs in UI/main thread&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Local-process Service — runs on invoking thread&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Local-process ContentProvider — runs on invoking thread&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Remote-process ContentProvider — runs on a thread in a thread pool on remote application process&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Remote-process (AIDL) Service — runs on …&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;If you ever wondered where the different Android components run by default:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Activity — always runs in UI/main thread&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Local-process Service — runs on invoking thread&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Local-process ContentProvider — runs on invoking thread&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Remote-process ContentProvider — runs on a thread in a thread pool on remote application process&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Remote-process (AIDL) Service — runs on a thread in a thread pool on remote application process&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Local-process BroadcastReceiver — always runs in UI/main thread&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Remote-process BroadcastReceiver — always runs in UI/main thread of remote process&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Link to GitHub repo with test project used for data:
&lt;a href="https://github.com/zachmccormick/AndroidComponentThreads"&gt;https://github.com/zachmccormick/AndroidComponentThreads&lt;/a&gt;&lt;/p&gt;</content><category term="software development"></category><category term="android"></category></entry><entry><title>Anonymous Shared Memory Library</title><link href="https://z11k.com/posts/2013/10/anonymous-shared-memory-library/" rel="alternate"></link><published>2013-10-02T00:00:00-04:00</published><updated>2013-10-02T00:00:00-04:00</updated><author><name>Zach McCormick</name></author><id>tag:z11k.com,2013-10-02:/posts/2013/10/anonymous-shared-memory-library/</id><summary type="html">&lt;p&gt;I created an Android Library project that supplies you with an AshmemBuffer object, that you can pass between
processes/applications via a service. Unfortunately, even though it is parcelable, you can’t pass it via an
Intent because Android doesn’t let you do that (it uses file descriptors under …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I created an Android Library project that supplies you with an AshmemBuffer object, that you can pass between
processes/applications via a service. Unfortunately, even though it is parcelable, you can’t pass it via an
Intent because Android doesn’t let you do that (it uses file descriptors under the hood).&lt;/p&gt;
&lt;p&gt;It has IO functions such as “writeByte”, “writeBytes”, “readByte”, and “readBytes”, with functions
for setting the pointer and getting the capacity. I plan to add more things later if anyone finds it useful. I just
wanted to make something cool so that people could use ashmem at the Java layer without having to mess around with
system calls/C/JNI/etc.&lt;/p&gt;
&lt;p&gt;Find it here: &lt;a href="https://github.com/zachmccormick/AshmemLibrary"&gt;https://github.com/zachmccormick/AshmemLibrary&lt;/a&gt;&lt;/p&gt;</content><category term="software development"></category><category term="android"></category></entry></feed>