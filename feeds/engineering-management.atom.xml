<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Thoughts and Discoveries of z11k - Engineering Management</title><link href="http://z11k.com/" rel="alternate"></link><link href="http://z11k.com/feeds/engineering-management.atom.xml" rel="self"></link><id>http://z11k.com/</id><updated>2018-01-02T00:00:00-05:00</updated><entry><title>Proverbs and Programming</title><link href="http://z11k.com/posts/2018/01/proverbs-and-programming/" rel="alternate"></link><published>2018-01-02T00:00:00-05:00</published><updated>2018-01-02T00:00:00-05:00</updated><author><name>Zach McCormick</name></author><id>tag:z11k.com,2018-01-02:/posts/2018/01/proverbs-and-programming/</id><summary type="html">&lt;p&gt;I collected a variety of proverbs over the course of 2017 that I thought had some insight into the world of
software engineering as well. This won’t be a horribly technical post, but a sort of fun rehash of experiences
I’ve had thus far working with engineers, teams …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I collected a variety of proverbs over the course of 2017 that I thought had some insight into the world of
software engineering as well. This won’t be a horribly technical post, but a sort of fun rehash of experiences
I’ve had thus far working with engineers, teams, product managers, CEOs, etc. over the years.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Don’t cross a bridge before you come to it&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Many systems that I’ve helped develop or analyze have had their fair share of &lt;strong&gt;IAbstractWindowFactory&lt;/strong&gt; or
&lt;strong&gt;IInterfacePrototype&lt;/strong&gt;-like classes and interfaces. I’ve learned that the most effective libraries and frameworks
are not the ones that account for everything possible from the beginning, but that consider the immediate needs
and the needs 3 to 6 months into the future. Beyond that, the unknowns are still so unknown — will the project
even exist then? Will the goals still be the same or similar? I have found that keeping frameworks and libraries
relatively simple, especially when beginning, make them more manageable and simple for others to use.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Don’t let the perfect become the enemy of the good&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the same vein as the previous point — working on frameworks and libraries (or even apps or other projects)
have significant potential to get out of hand quickly. Potential features are dreamed up, opportunities for plugins
and expansions run wild, and chances for over-engineering rocket skyward. I think that even for personal projects,
one should put on their product manager hat and really spend significant time away from their IDE and in Trello
(or Excel, or on post-its, or whatever you do). It’s incredibly easy to dream up a perfect project with thousands
of features, integrations, and capabilities, then never reach the finish line because of it.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The longest mile is the last mile home&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As a contractor who has helped a number of companies develop all kinds of applications, it’s easy to give clients
the false perspective that an application is almost finished or is really ahead of schedule. The first few important
features look like a ton of accomplished work, but at the end of the day, it’s the last few features and list
of improvements and bug fixes that end up taking a disproportionately long time.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Time is money&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I remember at a cyber security company that I once worked for — we had a meeting with everyone on the team — full
time or contract — for well over two or three hours planning out a research project. We debated technologies to
use, skills we’d need, subcontractors we’d need to hire. We left exhausted but glad that we’d cranked out all
of that good work. A week later (or so… my memory of the exact story is a bit fuzzy) the project was shelved. I
thought about all of that work we had done and what, approximately, each person at the table’s time was worth
that afternoon. At the end of that day, we’d spent well over $5000. If some more front-end product planning had
been done, we could have realized the impracticality of the potential product and saved quite a bit of money!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;More of these to come — I think I have well over 50 proverbs saved!&lt;/p&gt;</content><category term="proverbs"></category><category term="programming"></category><category term="engineering"></category><category term="teamwork"></category><category term="wisdom"></category></entry><entry><title>DevOps and Horizontal Scaling: Part 3</title><link href="http://z11k.com/posts/2017/07/devops-and-horizontal-scaling-part-3/" rel="alternate"></link><published>2017-07-25T00:00:00-04:00</published><updated>2017-07-25T00:00:00-04:00</updated><author><name>Zach McCormick</name></author><id>tag:z11k.com,2017-07-25:/posts/2017/07/devops-and-horizontal-scaling-part-3/</id><summary type="html">&lt;p&gt;Time to finish what we started: the last 4 points/suggestions on DevOps and Horizontal Scaling.&lt;/p&gt;
&lt;h3 id="manual-and-automated-code-review"&gt;Manual and Automated Code Review&lt;/h3&gt;
&lt;p&gt;Manual code reviews are easy — GitHub, Gerrit, and a host of other tools support this workflow. The gist is that
everyone’s pull requests need to be seen and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Time to finish what we started: the last 4 points/suggestions on DevOps and Horizontal Scaling.&lt;/p&gt;
&lt;h3 id="manual-and-automated-code-review"&gt;Manual and Automated Code Review&lt;/h3&gt;
&lt;p&gt;Manual code reviews are easy — GitHub, Gerrit, and a host of other tools support this workflow. The gist is that
everyone’s pull requests need to be seen and reviewed by a senior developer/manager/etc. before being merged into
the main development branch. The idea is that with many eyes on the code, problems can be spotted and solved early
while in context rather than later when the source of the issue can be harder to find. This is the first stage of
code reviews.&lt;/p&gt;
&lt;p&gt;The second stage (and final, in my eyes, as it’s not horribly complicated) of code reviews is automated code
reviews. This sounds (for now, until AI takes over) more complicated than it really is — ideally through a Jenkins
job or some other task runner, run a series of tools like pylint, jslint, Codacy, SonarQube, or even valgrind over
the PR. These tools produce all kinds of output, so they’ll need to be set up to provide the best kind of feedback
to the developer who made the PR. These types of tools do style analysis, static analysis, and even memory usage
analysis, and can find harder-to-spot problems or hotspots for null pointers, memory leaks, and other types of issues.&lt;/p&gt;
&lt;p&gt;As your team grows and every developer is handling a different kind of complexity, manual and automated code
reviews help to make sure that everything is being reviewed and understood before being deployed, both promoting
high-quality code and ensuring that if something were to go wrong, at least 2 developers should understand any
particular part of the codebase. Redundancy is your friend.&lt;/p&gt;
&lt;h3 id="scalability-through-paas-non-monolithic-architecture"&gt;Scalability through PaaS &amp;amp; Non-monolithic Architecture&lt;/h3&gt;
&lt;p&gt;For a very large number of systems, it makes a lot of sense to be using a PaaS (platform-as-a-service) provider
like Heroku, Google App Engine, or Engine Yard.  You don’t need to worry about knowing how to use Ubuntu/Debian or
Windows Server and you don’t need to understand how to use Docker or lxd or rkt or any number of new containerization
tools. These cloud PaaS providers just &lt;em&gt;work&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;(As an aside, You might look at Heroku’s pricing and think “but hosting a small server on AWS and managing
it myself will save me $200 per month!”, but in the end, I’m sure you will spend far more in paying employees
for hours spent setting up, debugging, and maintaining than the $200 you save each month.)&lt;/p&gt;
&lt;p&gt;In addition to the relieved knowledge pressure by using a PaaS instead of self-managed cloud servers (or on-premises
machines), you remove a specialization-of-function role of the “ops/IT/deployments person”, so if something isn’t
working right, more of your team will be effective in diagnosing and solving the problem. That’s a good thing!&lt;/p&gt;
&lt;p&gt;Paired with this is the use of a non-monolithic architecture. This one actually &lt;em&gt;adds&lt;/em&gt; complexity, but the payback
from this investment can be significant for some engineering teams. I’ll go into this further down this section.&lt;/p&gt;
&lt;p&gt;“Level One” of this concept is simply migrating your web services and backend resources over to a PaaS (I’ll
use Heroku as an example, due to my familiarity with it) like Heroku. All of a sudden, all of the components in
your deployments are accessible from one place, and often come with monitoring dashboards or inspection tools
baked right in. Settings are configured via environment variables (12factor.net) and logs are all piped into one
nice display. Cool!&lt;/p&gt;
&lt;p&gt;Getting to “Level Two” involves integrating your environments across something like a “pipeline” in
Heroku. This may exist for your PaaS provider or you may have to create this abstraction yourself, but the results
are fantastic. With Heroku’s Pipelines feature, you can integrate “Review Apps” (where GitHub Pull Requests are
automatically built and deployed to allow for manual testing and usage before merging into the development branch),
continuous integration (previously with an outside tool, but Heroku has released their own CI function), and staged
deployment. With these tools integrated, it becomes easy for developers (and honestly any other stakeholder) to
see and understand the flow of how new code and bug fixes make it down the pipe. This makes it much easier to add
new engineers and hit the ground running quickly.&lt;/p&gt;
&lt;p&gt;“Level Three” starts to introduce microservices or another non-monolithic architecture. Let me start by
saying &lt;strong&gt;microservices aren’t for every organization&lt;/strong&gt; — in fact, I highly recommend &lt;em&gt;against&lt;/em&gt; them for small
organizations or for prototypes/new projects. In a lot of cases like that, having a monolithic architecture is
going to keep things simple and keep your feature deployment cycle times much leaner.&lt;/p&gt;
&lt;p&gt;Adapting a non-monolithic architecture encourages a number of good practices: decoupling of components, increased
test-ability, and system isolation, to name a few. It also increases the complexity of deployments and where
things can go wrong. For a highly functional software engineering team, the benefits can often outweigh the
drawbacks — especially one with engineers devoted to the infrastructure itself (especially when cost and other
factors move you &lt;em&gt;away&lt;/em&gt; from PaaS offerings). With microservices used in a large project, it becomes easier for
new engineers to become acquainted with complex functionality quicker, because they (hopefully) no longer need to
understand far-reaching implications between systems or comprehend huge modules that span a wide swath of functional
domains. It also becomes easier to roll out new functionality: as the tight coupling between components decreases,
the risk of changes to one function breaking another function decreases as well.&lt;/p&gt;
&lt;h3 id="logging-instrumentation-and-monitoring"&gt;Logging, Instrumentation, and Monitoring&lt;/h3&gt;
&lt;p&gt;Logging seems pretty simple, and it is. Unless you have specific logging requirements, like from HIPAA, you
should pretty much spit out a lot of logs (but not too many… I know that’s vague) and save them for at least
long enough that it’s unlikely you need them again (and if it is likely, compress them and save them somewhere
cheap). Instrumentation and monitoring get a little more complex. Tools like New Relic or Dynatrace are instrumental
(ha!) to doing this successfully — they provide tools and dashboards that work with all types of projects to
analyze performance and detect problems that otherwise go undetected.&lt;/p&gt;
&lt;p&gt;The “Level One” of logging, instrumentation, and monitoring, is simply to have these tools hooked up. On Heroku,
I often use the free tiers of LogEntries and New Relic to start. The benefits you get from simply ensuring that
these are hooked up right are immense: you can immediately start seeing slow requests/queries or performance
bottlenecks in infrastructure, and you can begin to trace anomalous events back to specific sections of logs to
get better context when hunting down issues.&lt;/p&gt;
&lt;p&gt;“Level Two” (the final level, much like my view on code reviews) of this concept involves completing
full-system coverage using these tools, and configuring these tools for automated alerts (what I would call “true
monitoring”). Instead of going into the New Relic control panel when something is going wrong, it’s far better
to be warned by New Relic (via Slack, email, or a host of other options) that something is slowing down or &lt;em&gt;about&lt;/em&gt;
to go wrong.&lt;/p&gt;
&lt;p&gt;LogEntries and New Relic can both be set up to warn you about certain things: Apdex score warnings in New Relic
can warn you of overall app sluggishness, while New Relic’s Key Transactions feature can be set up to warn
you of sluggishness or errors in particular processes, such as customer checkout or PBX/customer service initial
response. LogEntries has a number of built-in pattern recognizers for Heroku, and StackOverflow and Google are
full of many, many more for detecting anomalous behavior or otherwise unseen errors.&lt;/p&gt;
&lt;p&gt;The benefit of these concepts to horizontal scaling (and general operation) of a team are plentiful: democratized
error handling and response — the whole team has the tools to analyze even the most technically complicated
of issues; continuous measuring and monitoring of performance —sluggishness introduced by new features or new
code is caught early and quickly; and performance quantification — product managers and executives can better
understand the impact of technical debt and architectural refactorings on the quality of the product.&lt;/p&gt;
&lt;h3 id="architecture-review-team"&gt;Architecture Review Team&lt;/h3&gt;
&lt;p&gt;The last item on my list of thing for building out a “DevOps culture” and a horizontally scalable team doesn’t
have levels: the Architecture Review Team. I think at large enough organizations, a team needs to exist, composed
of senior engineers of sub-teams and/or software architects with engineering managers (i.e. people with budgets).&lt;/p&gt;
&lt;p&gt;Ideally, all participants have a strong understanding of the impact that spending engineering time has on the
overall organization (i.e. it’s expensive), since software architecture at large firms is a balancing act of
figuring out the best architecture at the appropriate cost. During each meeting, which can happen as often as once
a month and as infrequently as every six months (though I like the happy medium of quarterly), the team should get
together and discuss the recent and upcoming major initiatives regarding new features, the state of the bug tracker,
the state of the performance monitoring tools, and the infrastructure budget (money spent on servers, software, etc.).&lt;/p&gt;
&lt;p&gt;The goal of the meeting is to make sure everyone has an understanding of the macro state of the system, and to allow
for discussions about issues like tech debt, refactoring, hiring, and infrastructure changes. It should also prompt
discussions about future software initiatives — architects and team leads will have a good idea about the scope
of new initiatives and whether or not it will have a deleterious effect on the existing system without additional
planning or a round of refactoring first.&lt;/p&gt;
&lt;h3 id="how-do-you-do-it"&gt;How do you do it?&lt;/h3&gt;
&lt;p&gt;As I mentioned in the last post: careful planning and working with the engineering team and outside product
owners/users is critical to implementing these steps successfully. This is actually something that I love to consult
with companies about — shoot me an e-mail at &lt;a href="mailto:z@z11k.com"&gt;z@z11k.com&lt;/a&gt; to
get a bit more information about that. I love traveling and will gladly come to you wherever you are to help your
company succeed in implementing these tools.&lt;/p&gt;</content><category term="software development"></category><category term="engineering management"></category><category term="teams"></category><category term="tools"></category></entry><entry><title>DevOps and Horizontal Scaling: Part 2</title><link href="http://z11k.com/posts/2017/06/devops-and-horizontal-scaling-part-2/" rel="alternate"></link><published>2017-06-29T00:00:00-04:00</published><updated>2017-06-29T00:00:00-04:00</updated><author><name>Zach McCormick</name></author><id>tag:z11k.com,2017-06-29:/posts/2017/06/devops-and-horizontal-scaling-part-2/</id><summary type="html">&lt;p&gt;Lets talk about the first 3 of the 7 tools I described in the last post!&lt;/p&gt;
&lt;h2 id="product-management-involving-feature-flags-switches-and-samples"&gt;Product Management involving feature flags, switches, and samples&lt;/h2&gt;
&lt;p&gt;Product management has a simple sort of flow to it — tickets and feature requests come in, things are prioritized
between managers of different teams, and the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Lets talk about the first 3 of the 7 tools I described in the last post!&lt;/p&gt;
&lt;h2 id="product-management-involving-feature-flags-switches-and-samples"&gt;Product Management involving feature flags, switches, and samples&lt;/h2&gt;
&lt;p&gt;Product management has a simple sort of flow to it — tickets and feature requests come in, things are prioritized
between managers of different teams, and the end result gets put into a pipeline for engineers to work on. Ideally
the flow is smooth and reliable, with refactoring and testing getting their fair share of time as well. In practice,
most organizations are way behind where they’d like to be, so these things get de-prioritized.&lt;/p&gt;
&lt;p&gt;In my opinion, based on my time either working at or working for a number of companies that develop software, the
first box to check in product management is just having a workflow that can be tracked — do you know the mean
time between a bug being reported and it being fixed? What about a simple feature request? Obviously the variables
involved make it so that these numbers don’t fit into a nice and tidy box like “how many widgets did our old
factory produce, and how many does our new one produce?”, but they’re still important to keep track of. I deem
this “level one” of product management.&lt;/p&gt;
&lt;p&gt;“Level two” of product management is not much harder — just make sure the rest of the organization is aware
of bug fixes and finished features. Some organizations use JIRA, which can be set up in a way to notify requestors
about the status of their request, or post updates to Slack when releases get pushed to production. Automating
this process is “level two”. You’d be surprised just how happy people get when they have some semblance of
an idea of what’s going on in the engineering department related to their needs!&lt;/p&gt;
&lt;p&gt;“Level three” of product management, however, involves adding a significant QA process to the mix. Putting
aside the “developer-centric” parts of QA (code reviews, testing, CI), this should involve at least some
process at the roll-out level. Maybe you have a QA person or QA team that evaluates features in your development
and staging deployments — this is a great first step — but changing the culture to liberally use feature flags,
feature switches, and sampled roll-outs will bring you to “level three”. If you roll something out that has
a deleterious effect, you ought to be able to quickly roll it back out. With feature flags and switches, this is
easy to do without redeploying or merging hot-fixes into your production environment. If you aren’t sure about a
particular feature — especially revamps of UI/UX — you’ll learn to love sampling your releases — releasing
only to select sets of users at once.&lt;/p&gt;
&lt;p&gt;These capabilities give you real-time control over your product allowing your engineering team to continue with
their normal flow instead of needing to respond to emergencies on a regular basis. This allows you to keep adding
engineers to your team while remaining efficient.&lt;/p&gt;
&lt;h2 id="consistent-realistic-development-and-staging-environments"&gt;Consistent, Realistic Development (and Staging) Environments&lt;/h2&gt;
&lt;p&gt;Consistent environments in your development and staging environment (hosted) and for your engineers locally is
key to continued success as more people work on more and more complex features.&lt;/p&gt;
&lt;p&gt;“Level one” here is simply having a procedure for engineers to run a production-like environment on their own
machine. Without this, deployments are dangerous and feature release can be troublesome. The technologies don’t
need to mirror each other — for instance you might actually be deploying load-balanced Docker containers to
a Kubernetes cluster, but locally you can run manage.py runserver and achieve 99% the same result — but they
should be similar enough that you rarely run into issues in production that you don’t have locally.&lt;/p&gt;
&lt;p&gt;“Level two” is where architecture and the “ops” part of DevOps comes into play — ideally your system
should operate as stateless-ly as possible. If you have your database running separate from your application server,
your ElasticSearch cluster in its own world, and your cache running in its own universe, then you’re well on
your way to achieving “level two”. That kind of separation allows you to test your services independently and
substitute out equivalent (but less complex/costly) services locally or on development/staging environments.&lt;/p&gt;
&lt;p&gt;The last, and often neglected, level of Realistic Environments is having similar (but sanitized) data available
in different environments and locally. It’s tough to work on pagination algorithms, document search features,
or speeding up high-throughput views with a cache without having sufficient or realistic data to test locally. This
is something else that lends itself well to automation on the “ops” side of DevOps. A daily/weekly/etc. Jenkins
job that takes a database or cache backup, sanitizes out sensitive information, and stores it in S3 can save you
from countless production deployments solving problems that only exist on production.&lt;/p&gt;
&lt;p&gt;With these strategies in place, “tribal knowledge” of how systems work starts to go away and onboarding new
engineers becomes a piece of cake, allowing you to scale your team more effectively.&lt;/p&gt;
&lt;h2 id="extensive-automated-testing-and-continuous-integration"&gt;Extensive Automated Testing and Continuous Integration&lt;/h2&gt;
&lt;p&gt;The last of the three tools I’m going to talk about in this post is testing. It may seem obvious, given the
number of “How to Test using JUnit” talks at local meetups, but testing should be your best friend. Countless
bad deployments are saved due to effective unit and integration tests every day.&lt;/p&gt;
&lt;p&gt;The first hurdle is simply test coverage, and reporting on test coverage. A simple and effective KPI (though remember,
engineering can be fuzzy and full of variables, so you can’t worship the KPI) is test coverage. Several derivatives
exist, such as “test coverage of critical paths”, but having a few of these reported on weekly can be a great
tool to keep the team abreast of how well-tested your code is. This might involve unit tests, integration tests,
load tests, or more, but the first step is simply having the tests available and running/reporting on them on occasion.&lt;/p&gt;
&lt;p&gt;The second hurdle is automating those tests — this could be as simple as a Jenkins job that runs ./manage.py
test on every PR and before deploying to development/staging environments. It may involve a suite of tests,
or involve more complex testing frameworks like Cucumber using Behavior-driven Development. Knowing when tests
fail and responding to them quickly and effectively is “level two” in this realm. I’m a big fan of using
Cucumber or Behave — two Python frameworks for BDD — because of the ability to create cross-products of tests
easily. This might be something like end-to-end tests of eCommerce, where each product has a suite of 50 tests
run on it, and simply adding 3–4 additional product codes actually runs an additional 150–200 tests.&lt;/p&gt;
&lt;p&gt;The final stage of effective automated testing is actually less about testing and more about culture/product
management. It takes more time to implement, being a culture thing, but part of the bug reporting/fixing process
should involve unit and integration tests to prevent regressions from occurring more than once. Users are frustrated
when things break, but they’re even more frustrate when the same thing breaks a second time. This one is a little
more loose, as there isn’t a framework you can add or a tool you can use to solve it instantly, but is a great
goal to try to achieve when it comes to automated testing.&lt;/p&gt;
&lt;p&gt;With this kind of testing in place, engineers don’t need to know every last detail and caveat of system
functionality — they can just start coding features and fixing bugs in their first couple of weeks.&lt;/p&gt;
&lt;h2 id="how-do-you-do-it"&gt;How do you do it?&lt;/h2&gt;
&lt;p&gt;Careful planning and working with the engineering team and outside product owners/users is critical to implementing
these steps successfully. This is actually something that I love to consult with companies about — shoot me an
e-mail at &lt;a href="mailto:z@z11k.com"&gt;z@z11k.com&lt;/a&gt; to get a bit more information about that. I
love traveling and will gladly come to you wherever you are to help your company succeed in implementing these tools.&lt;/p&gt;
&lt;h2 id="what-next"&gt;What next?&lt;/h2&gt;
&lt;p&gt;In my next post, I’ll go over the last 4 tools to implementing a DevOps culture to enable Horizontal Scaling
within an engineering organization. Stay tuned!&lt;/p&gt;</content><category term="software development"></category><category term="engineering management"></category><category term="teams"></category><category term="tools"></category></entry><entry><title>DevOps and Horizontal Scaling: Part 1</title><link href="http://z11k.com/posts/2017/06/devops-and-horizontal-scaling-part-1/" rel="alternate"></link><published>2017-06-28T00:00:00-04:00</published><updated>2017-06-28T00:00:00-04:00</updated><author><name>Zach McCormick</name></author><id>tag:z11k.com,2017-06-28:/posts/2017/06/devops-and-horizontal-scaling-part-1/</id><summary type="html">&lt;p&gt;How do engineering departments scale beyond teams of 10–15 people and stay nimble?&lt;/p&gt;
&lt;p&gt;Let’s imagine scaling an engineering department that starts with 3 employees and an “efficiency factor”
of 100%(bear with me). You hire a couple more experienced engineers, start having daily stand-ups and a weekly
planning …&lt;/p&gt;</summary><content type="html">&lt;p&gt;How do engineering departments scale beyond teams of 10–15 people and stay nimble?&lt;/p&gt;
&lt;p&gt;Let’s imagine scaling an engineering department that starts with 3 employees and an “efficiency factor”
of 100%(bear with me). You hire a couple more experienced engineers, start having daily stand-ups and a weekly
planning meeting and compared to your previous “efficiency rating”, you’re now outputting the work of 5
engineers at 90% efficiency — not bad!&lt;/p&gt;
&lt;p&gt;The business starts to need more things, and managing tickets and support requests start to become a task of their
own, so your efficiency drops to around 75%. You hire a product manager and a support engineer, bringing you up
to 7 employees and back to an efficiency rating around 95%(your core engineers are no longer managing tickets or
support requests).&lt;/p&gt;
&lt;p&gt;Time continues and business is good, but as the features keep piling on — internal CRM features, ERP features for
driving workflow, eCommerce features for increasing sales — things start to break down. Deployments aren’t as
smooth or fast as they used to be, minor feature additions are breaking existing features, and bug fixes are slow
due to the manual testing involved.&lt;/p&gt;
&lt;p&gt;You’re smart and have faced these types of multifaceted problems before, so you focus on an easy win:
infrastructure. You hire an infrastructure engineer to sort out deployments and scaling. While she’s getting
integrated with the team, you start a feature freeze to work on better testing. Things seem to be looking up —
engineering KPIs are stabilizing and people &lt;em&gt;feel&lt;/em&gt; good about it.&lt;/p&gt;
&lt;p&gt;Time moves on, you have occasional cycles of writing lots of tests, and you start to accept the occasional
deployment hiccup as a fact of life. You aren’t too concerned with that engineering efficiency rating anymore
(which is hovering around 50%) because the product has just gotten larger and that’s how software works —
isn’t the phrase “oh look at that big and slow-moving company” just normal?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It does not have to be!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is where complacency stops engineering teams from achieving the very possible horizontal scaling that many
hugely successful companies achieve. With the right investment in &lt;strong&gt;DevOps&lt;/strong&gt;, horizontal scaling is absolutely
possible beyond 10, 20, 50, or even 100 engineers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DevOps&lt;/strong&gt; is a phrase that gets used a lot for a lot of different reasons, so let’s start with a couple definitions:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DevOps&lt;/strong&gt;— a portmanteau of “development” and “operations”. A combination of the engineers who write new
features and the IT staff that keep the servers running. DevOps is an umbrella term for a culture of collaboration
between these participants in the organizational software lifecycle.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Horizontal Scaling&lt;/strong&gt; — the ability to scale by adding more resources to your pool of resources. Note that
this isn’t vertical scaling — adding more powerful resources (i.e. a superstar engineer), but simply adding
more resources.&lt;/p&gt;
&lt;p&gt;I like to think of implementing “the DevOps culture” as something like an organization adopting Six Sigma
or Kaizen — it’s a set of tools with a cohesive purpose that you can pick and choose from to address your
needs. What are those tools, then?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Product Management involving feature flags, switches, and samples&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Consistent, Realistic Development (and Staging) Environments&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Extensive Automated Testing and Continuous Integration&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Manual &lt;strong&gt;and&lt;/strong&gt; Automated Code Review&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scalability through PaaS &amp;amp; Non-monolithic Architecture&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Logging, Instrumentation, and Monitoring&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Architecture Review Team&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In my next post, I’ll go over each of these tools, tying them back to the ultimate goal of keeping “engineering
efficiency” high while allowing an organization to scale horizontally in engineering team size.&lt;/p&gt;</content><category term="software development"></category><category term="engineering management"></category><category term="teams"></category><category term="tools"></category></entry></feed>